# âœ… Event-Driven Features - Deployment Complete

## Overview
Successfully deployed event-driven microservices architecture with Kafka, Dapr, and two specialized microservices for handling reminders and recurring tasks.

---

## ğŸ—ï¸ Architecture

### Event-Driven Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Backend (FastAPI)                        â”‚
â”‚  - Publishes events to Kafka via Dapr                          â”‚
â”‚  - Topics: "reminders", "task-events"                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Kafka + Zookeeper                            â”‚
â”‚  - Message broker for event streaming                          â”‚
â”‚  - Persistent event storage                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                            â”‚
             â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Notification Service  â”‚    â”‚  Recurring Task Service          â”‚
â”‚  - Subscribes to       â”‚    â”‚  - Subscribes to                 â”‚
â”‚    "reminders" topic   â”‚    â”‚    "task-events" topic           â”‚
â”‚  - Sends email         â”‚    â”‚  - Creates task instances        â”‚
â”‚    notifications       â”‚    â”‚  - Updates next occurrence       â”‚
â”‚  - Port: 8001          â”‚    â”‚  - Port: 8002                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Components Deployed

### 1. Kafka (Message Broker)
**Status:** âœ… Running
**Pod:** `kafka-0` (1/1 Running)
**Service:** `kafka-service:9092`
**Purpose:** Event streaming and message persistence

**Configuration:**
- Single broker deployment
- Auto-create topics enabled
- Replication factor: 1
- Memory: 200Mi-400Mi
- CPU: 100m-300m

### 2. Zookeeper (Coordination Service)
**Status:** âœ… Running
**Pod:** `zookeeper-0` (1/1 Running)
**Service:** `zookeeper-service:2181`
**Purpose:** Kafka cluster coordination

**Configuration:**
- Single node deployment
- Client port: 2181
- Memory: 128Mi-256Mi
- CPU: 50m-200m

### 3. Notification Service
**Status:** âœ… Running (2/2 containers)
**Pod:** `notification-service-5cf5cf9ff5-s2nrb`
**Image:** `devhasnainraza/notification-service:v1.1`
**Service:** `notification-service:8001`
**Dapr App ID:** `notification-service`

**Features:**
- Subscribes to "reminders" topic
- Sends email notifications via backend
- Processes reminder events
- Dapr sidecar for pub/sub

**Subscriptions:**
```json
{
  "pubsubname": "kafka-pubsub",
  "topic": "reminders",
  "route": "/reminders"
}
```

### 4. Recurring Task Service
**Status:** âœ… Running (2/2 containers)
**Pod:** `recurring-task-service-7c846bbd47-s7pgv`
**Image:** `devhasnainraza/recurring-task-service:v1.1`
**Service:** `recurring-task-service:8002`
**Dapr App ID:** `recurring-task-service`

**Features:**
- Subscribes to "task-events" topic
- Creates task instances from recurring tasks
- Calculates next occurrence dates
- Updates recurring task metadata
- Dapr sidecar for pub/sub

**Subscriptions:**
```json
{
  "pubsubname": "kafka-pubsub",
  "topic": "task-events",
  "route": "/task-events"
}
```

### 5. Dapr Components

**Kafka Pub/Sub Component:**
- Name: `kafka-pubsub`
- Type: `pubsub.kafka`
- Broker: `kafka-service:9092`
- Consumer Group: `todo-app-group`
- Scopes: backend, notification-service, recurring-task-service

**State Store Component:**
- Name: `statestore`
- Type: `state.redis`
- Scope: backend only (microservices don't need state)

---

## ğŸ”§ Issues Fixed

### Issue 1: Redis State Store Not Available
**Problem:** Dapr sidecars failing because they couldn't connect to Redis state store

**Root Cause:**
- Statestore component configured for all services
- Redis not deployed in cluster
- Microservices don't actually need state storage

**Solution:**
- Updated `kubernetes/dapr/statestore.yaml`
- Removed notification-service and recurring-task-service from scopes
- Only backend uses state store now

**Files Modified:**
```yaml
# kubernetes/dapr/statestore.yaml
scopes:
- backend  # Removed microservices from scope
```

### Issue 2: Dapr Subscription Endpoint Method Mismatch
**Problem:** Dapr making GET requests to `/dapr/subscribe`, but endpoint defined as POST

**Root Cause:**
- Microservices defined subscription endpoint as `@app.post("/dapr/subscribe")`
- Dapr specification requires GET method for subscription discovery

**Solution:**
- Changed both microservices to use `@app.get("/dapr/subscribe")`
- Rebuilt and pushed images with v1.1 tag
- Updated deployment manifests to use v1.1

**Files Modified:**
- `services/notification-service/main.py` (line 243)
- `services/recurring-task-service/main.py` (line 318)
- `kubernetes/notification-service-deployment.yaml` (image tag)
- `kubernetes/recurring-task-service-deployment.yaml` (image tag)

### Issue 3: Docker Image Caching
**Problem:** Kubernetes pulling old cached images despite imagePullPolicy: Always

**Root Cause:**
- Using "latest" tag causes Docker Hub caching issues
- Kubernetes may use cached image even with Always pull policy

**Solution:**
- Tagged images with version numbers (v1.1)
- Updated deployment manifests to use versioned tags
- Forced fresh image pull with specific versions

---

## ğŸ“Š Current Status

### All Services Running
```
âœ… Kafka:                  1/1 Running
âœ… Zookeeper:              1/1 Running
âœ… Notification Service:   2/2 Running (app + Dapr)
âœ… Recurring Task Service: 2/2 Running (app + Dapr)
âœ… Backend:                1/1 Running
âœ… Frontend:               1/1 Running
```

### Subscriptions Active
```
âœ… notification-service â†’ kafka-pubsub/reminders
âœ… recurring-task-service â†’ kafka-pubsub/task-events
```

### Health Checks
```bash
# Notification Service
kubectl logs -n todo-app -l app=notification-service -c notification-service
# Output: "GET /dapr/subscribe HTTP/1.1" 200 OK âœ…

# Recurring Task Service
kubectl logs -n todo-app -l app=recurring-task-service -c recurring-task-service
# Output: "GET /dapr/subscribe HTTP/1.1" 200 OK âœ…
```

---

## ğŸ§ª Testing Event-Driven Features

### Prerequisites
1. All services running (check with `kubectl get pods -n todo-app`)
2. User signed in to application
3. Backend configured to publish events

### Test 1: Reminder Notifications

**Step 1: Create a Task with Reminder**
```bash
# Via Frontend UI
1. Go to http://34.93.40.176/tasks
2. Create a new task
3. Set due date to tomorrow
4. Enable reminders
```

**Step 2: Backend Publishes Event**
When reminder time arrives, backend should publish event to Kafka:
```python
# Backend code (example)
await dapr_client.publish_event(
    pubsub_name="kafka-pubsub",
    topic_name="reminders",
    data={
        "event_id": "...",
        "event_type": "reminder.due",
        "user_id": user_id,
        "reminder_id": reminder_id,
        "task_id": task_id,
        "data": {
            "task_title": "...",
            "remind_at": "...",
            "notification_channels": "email"
        }
    }
)
```

**Step 3: Verify Notification Service Processes Event**
```bash
kubectl logs -n todo-app -l app=notification-service -c notification-service --tail=50
# Should see: "Processing reminder event: ..."
# Should see: "Email notification sent successfully"
```

### Test 2: Recurring Task Instance Creation

**Step 1: Create Recurring Task**
```bash
# Via Frontend UI
1. Go to http://34.93.40.176/recurring-tasks
2. Create a new recurring task
3. Set pattern: Daily, Interval: 1
4. Set start date: Today
```

**Step 2: Backend Publishes Event**
When next occurrence time arrives, backend should publish event:
```python
# Backend code (example)
await dapr_client.publish_event(
    pubsub_name="kafka-pubsub",
    topic_name="task-events",
    data={
        "event_id": "...",
        "event_type": "recurring_task.due",
        "user_id": user_id,
        "recurring_task_id": recurring_task_id,
        "data": {
            "title": "...",
            "recurrence_pattern": "daily",
            "recurrence_interval": 1,
            "next_occurrence": "...",
            "is_active": True
        }
    }
)
```

**Step 3: Verify Recurring Task Service Creates Instance**
```bash
kubectl logs -n todo-app -l app=recurring-task-service -c recurring-task-service --tail=50
# Should see: "Processing recurring task event: ..."
# Should see: "Created task instance from recurring task"
# Should see: "Updated next occurrence for recurring task"
```

**Step 4: Verify New Task Created**
```bash
# Check tasks list in frontend
# Should see new task instance with due date = next occurrence
```

---

## ğŸ”Œ Backend Integration

### Required Backend Changes

The backend needs to be updated to publish events to Kafka via Dapr. Here's what needs to be added:

### 1. Install Dapr SDK
```bash
pip install dapr
```

### 2. Initialize Dapr Client
```python
# backend/src/main.py
from dapr.clients import DaprClient

# Initialize Dapr client
dapr_client = DaprClient()
```

### 3. Publish Reminder Events
```python
# backend/src/api/reminders.py
from dapr.clients import DaprClient
import json
from datetime import datetime

async def publish_reminder_event(reminder, task, user):
    """Publish reminder event to Kafka via Dapr."""
    try:
        with DaprClient() as client:
            event_data = {
                "event_id": f"reminder-{reminder.id}-{datetime.utcnow().timestamp()}",
                "event_type": "reminder.due",
                "timestamp": datetime.utcnow().isoformat(),
                "user_id": user.id,
                "reminder_id": reminder.id,
                "task_id": task.id,
                "data": {
                    "reminder_id": reminder.id,
                    "task_id": task.id,
                    "task_title": task.title,
                    "remind_at": reminder.remind_at.isoformat(),
                    "reminder_type": reminder.reminder_type,
                    "notification_channels": reminder.notification_channels
                }
            }

            # Publish to Kafka via Dapr
            client.publish_event(
                pubsub_name="kafka-pubsub",
                topic_name="reminders",
                data=json.dumps(event_data),
                data_content_type="application/json"
            )

            logger.info(f"Published reminder event: {event_data['event_id']}")

    except Exception as e:
        logger.error(f"Failed to publish reminder event: {str(e)}")
```

### 4. Publish Recurring Task Events
```python
# backend/src/api/recurring_tasks.py
from dapr.clients import DaprClient
import json
from datetime import datetime

async def publish_recurring_task_event(recurring_task, user):
    """Publish recurring task event to Kafka via Dapr."""
    try:
        with DaprClient() as client:
            event_data = {
                "event_id": f"recurring-task-{recurring_task.id}-{datetime.utcnow().timestamp()}",
                "event_type": "recurring_task.due",
                "timestamp": datetime.utcnow().isoformat(),
                "user_id": user.id,
                "recurring_task_id": recurring_task.id,
                "data": {
                    "recurring_task_id": recurring_task.id,
                    "title": recurring_task.title,
                    "description": recurring_task.description,
                    "status": recurring_task.status,
                    "priority": recurring_task.priority,
                    "recurrence_pattern": recurring_task.recurrence_pattern,
                    "recurrence_interval": recurring_task.recurrence_interval,
                    "next_occurrence": recurring_task.next_occurrence.isoformat(),
                    "is_active": recurring_task.is_active
                }
            }

            # Publish to Kafka via Dapr
            client.publish_event(
                pubsub_name="kafka-pubsub",
                topic_name="task-events",
                data=json.dumps(event_data),
                data_content_type="application/json"
            )

            logger.info(f"Published recurring task event: {event_data['event_id']}")

    except Exception as e:
        logger.error(f"Failed to publish recurring task event: {str(e)}")
```

### 5. Add Scheduled Jobs

Create a background scheduler to check for due reminders and recurring tasks:

```python
# backend/src/scheduler.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from datetime import datetime, timedelta
from sqlmodel import select

scheduler = AsyncIOScheduler()

async def check_due_reminders():
    """Check for due reminders and publish events."""
    now = datetime.utcnow()

    # Query reminders that are due
    statement = select(Reminder).where(
        Reminder.remind_at <= now,
        Reminder.is_dismissed == False
    )
    reminders = session.exec(statement).all()

    for reminder in reminders:
        task = session.get(Task, reminder.task_id)
        user = session.get(User, task.user_id)

        # Publish event
        await publish_reminder_event(reminder, task, user)

        # Mark as processed
        reminder.is_dismissed = True
        session.commit()

async def check_due_recurring_tasks():
    """Check for due recurring tasks and publish events."""
    now = datetime.utcnow()

    # Query recurring tasks that are due
    statement = select(RecurringTask).where(
        RecurringTask.next_occurrence <= now,
        RecurringTask.is_active == True
    )
    recurring_tasks = session.exec(statement).all()

    for recurring_task in recurring_tasks:
        user = session.get(User, recurring_task.user_id)

        # Publish event
        await publish_recurring_task_event(recurring_task, user)

# Schedule jobs
scheduler.add_job(check_due_reminders, 'interval', minutes=1)
scheduler.add_job(check_due_recurring_tasks, 'interval', minutes=1)

# Start scheduler
scheduler.start()
```

### 6. Update Backend Deployment

Add Dapr annotations to backend deployment:

```yaml
# kubernetes/backend-deployment.yaml
annotations:
  dapr.io/enabled: "true"
  dapr.io/app-id: "backend"
  dapr.io/app-port: "8000"
  dapr.io/log-level: "info"
```

---

## ğŸ“ Files Modified

### Kubernetes Manifests
- `kubernetes/kafka/kafka-deployment.yaml` - Kafka and Zookeeper deployment
- `kubernetes/dapr/pubsub.yaml` - Kafka pub/sub component
- `kubernetes/dapr/statestore.yaml` - State store scopes updated
- `kubernetes/notification-service-deployment.yaml` - Image tag updated to v1.1
- `kubernetes/recurring-task-service-deployment.yaml` - Image tag updated to v1.1

### Microservices Code
- `services/notification-service/main.py` - Fixed subscription endpoint (POST â†’ GET)
- `services/recurring-task-service/main.py` - Fixed subscription endpoint (POST â†’ GET)

### Docker Images
- `devhasnainraza/notification-service:v1.1` - Built and pushed
- `devhasnainraza/recurring-task-service:v1.1` - Built and pushed

---

## ğŸ¯ Next Steps

### 1. Backend Integration (Required)
- [ ] Install Dapr SDK in backend
- [ ] Add event publishing functions
- [ ] Create background scheduler for checking due items
- [ ] Add Dapr annotations to backend deployment
- [ ] Test end-to-end event flow

### 2. Email Service Configuration (Required)
- [ ] Configure SMTP settings in backend
- [ ] Set up email templates
- [ ] Test email delivery

### 3. Monitoring & Observability (Recommended)
- [ ] Set up Kafka monitoring (Kafka Manager or Kafdrop)
- [ ] Add Dapr dashboard for debugging
- [ ] Configure logging aggregation
- [ ] Set up alerts for failed events

### 4. Production Readiness (Recommended)
- [ ] Increase Kafka replicas for high availability
- [ ] Add persistent volumes for Kafka data
- [ ] Configure resource limits based on load
- [ ] Set up backup and disaster recovery
- [ ] Implement dead letter queues for failed events

---

## ğŸ› Troubleshooting

### Check Microservice Logs
```bash
# Notification Service
kubectl logs -n todo-app -l app=notification-service -c notification-service --tail=50

# Recurring Task Service
kubectl logs -n todo-app -l app=recurring-task-service -c recurring-task-service --tail=50
```

### Check Dapr Sidecar Logs
```bash
# Notification Service Dapr
kubectl logs -n todo-app -l app=notification-service -c daprd --tail=50

# Recurring Task Service Dapr
kubectl logs -n todo-app -l app=recurring-task-service -c daprd --tail=50
```

### Check Kafka Logs
```bash
kubectl logs -n todo-app kafka-0 --tail=50
```

### Check Zookeeper Logs
```bash
kubectl logs -n todo-app zookeeper-0 --tail=50
```

### Verify Subscriptions
```bash
# Should return subscription configuration
curl http://notification-service:8001/dapr/subscribe
curl http://recurring-task-service:8002/dapr/subscribe
```

### Test Kafka Connectivity
```bash
# Exec into Kafka pod
kubectl exec -it kafka-0 -n todo-app -- bash

# List topics
kafka-topics --list --bootstrap-server localhost:9092

# Produce test message
kafka-console-producer --broker-list localhost:9092 --topic reminders

# Consume messages
kafka-console-consumer --bootstrap-server localhost:9092 --topic reminders --from-beginning
```

---

## ğŸ“Š Resource Usage

### Current Allocation
```
Kafka:                 200Mi-400Mi RAM, 100m-300m CPU
Zookeeper:             128Mi-256Mi RAM, 50m-200m CPU
Notification Service:  128Mi-256Mi RAM, 50m-150m CPU
Recurring Task:        128Mi-256Mi RAM, 50m-150m CPU
```

### Recommended for Production
```
Kafka:                 1Gi-2Gi RAM, 500m-1000m CPU (3 replicas)
Zookeeper:             512Mi-1Gi RAM, 250m-500m CPU (3 replicas)
Notification Service:  256Mi-512Mi RAM, 100m-300m CPU (2 replicas)
Recurring Task:        256Mi-512Mi RAM, 100m-300m CPU (2 replicas)
```

---

## ğŸ‰ Success Metrics

### Deployment Complete
- âœ… Kafka and Zookeeper running
- âœ… Both microservices deployed with Dapr sidecars
- âœ… Subscriptions active and working
- âœ… All pods healthy (2/2 Running)
- âœ… No errors in logs
- âœ… Ready for backend integration

### Event-Driven Architecture Benefits
- ğŸš€ **Scalability:** Microservices can scale independently
- ğŸ”„ **Reliability:** Kafka provides message persistence and replay
- ğŸ¯ **Decoupling:** Services communicate via events, not direct calls
- ğŸ“Š **Observability:** Event logs provide audit trail
- ğŸ›¡ï¸ **Resilience:** Failed events can be retried automatically

---

**Status:** âœ… EVENT-DRIVEN FEATURES DEPLOYED AND READY
**Date:** February 13, 2026
**Next:** Backend integration to publish events

**All microservices are running and subscribed to Kafka topics. Ready for end-to-end testing once backend integration is complete!** ğŸš€
